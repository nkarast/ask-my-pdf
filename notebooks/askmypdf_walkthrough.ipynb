{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AskMyPDF \n",
    "\n",
    "AskMyPDF is a simple RAG project that uses a *local* LLM to embed and generate responses to a user question using as context the PDF document.\n",
    "\n",
    "The application is built around \n",
    "- `langchain` runnables\n",
    "- `llama_cpp_python` which allows the use of GGUF models over a C++ compiled executor\n",
    "- `ChromaDB` as a vector store for the embeddings\n",
    "- `streamlit` to develop the basic UI and run it in a local server.\n",
    "\n",
    "This notebook goes through the basic steps, simplifying where necessary to keep it short and notebook- and walkthrough-friendly.\n",
    "\n",
    "## 0. Getting started\n",
    "\n",
    "For this notebook to be useful, make sure you have the following dependencies:\n",
    "- `langchain` both `core` and `community`\n",
    "- `llama-cpp-python`\n",
    "- `chromadb`\n",
    "- `pypdf`\n",
    "- `gradio`: the last cell on the notebook uses a `Gradio` interface for a quick in-notebook experimentation instead of a `streamlit` implementation.\n",
    "\n",
    "\n",
    "**N.B. #1:** In this notebook we are using the `Llama-2-7b-chat` model quantized and provided by [`TheBloke`](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF) to both embed the document and generate the response. In terms of the `llama-cpp-python` dependecy, it looks like the latest version does not support generation of embeddings using models with no pool layers, throwing a segfault. A quick solution is to downgrade to `llama-cpp-python==0.2.47`\n",
    "\n",
    "**N.B. #2:** To use your local GPU for inference speed-up you need to export some env variables before pip installing `llama_cpp_python`. Follow the documentation based on your platform and your needs. \n",
    "\n",
    "For example, on an Apple Silicon platform, to use the Metal acceleration\n",
    "\n",
    "```bash\n",
    "export CMAKE_ARGS=\"-DLLAMA_METAL=on\"\n",
    "export FORCE_CMAKE=1\n",
    "pip install llama-cpp-python==0.2.47 --no-cache-dir\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting the Model and PDF Paths\n",
    "\n",
    "Here I am storing the `.gguf` of the model and the `.pdf` file in a local directory. You can change the path to point to your own files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = conf[\"MODEL_PATH\"]\n",
    "PDF_PATH = conf[\"PDF_PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- MODEL: llama-2-13b-chat.Q4_K_M.gguf\n",
      "- PDF: CERN-ACC-NOTE-2019-0046.pdf\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"- MODEL: {MODEL_PATH.split('/')[-1]}\\n- PDF: {PDF_PATH.split(\"/\")[-1]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the PDF and Split\n",
    "\n",
    "Here simply load the PDF with the PyPDFLoader. We do a *quick and dirty* `load_and_split` that splits the document at 1 chunk per page. Of course, different splitters can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.pdf.PyPDFLoader object at 0x10571c4c0>\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(PDF_PATH)\n",
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`pages`\n",
      "-------\n",
      " 1. has length 17\n",
      " 2. is of type <class 'list'> and \n",
      " 3. each index is of type <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "pages = loader.load_and_split()\n",
    "print(\n",
    "    f\"\"\"`pages`\\n-------\\n 1. has length {len(pages)}\\n 2. is of type {type(pages)} and \\n 3. each index is of type {type(pages[0])}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`embeddings` is of type <class 'langchain_community.embeddings.llamacpp.LlamaCppEmbeddings'>\n"
     ]
    }
   ],
   "source": [
    "embeddings = LlamaCppEmbeddings(model_path=MODEL_PATH, n_gpu_layers=-1, verbose=False)\n",
    "print(f\"`embeddings` is of type {type(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector Store\n",
    "\n",
    "Create a local (in memory) vector store. Each document is a single PDF page. After creating the vector store set it up as a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This embeds the document.. Will take few seconds depending on local hardware.\n",
    "docs = Chroma.from_documents(pages, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting the document as a retriever you can specify what is the search type you want and what are the search parameters. For example here,let's use the default parameters (similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docs.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly test the retriever. My default document is a technical document on a Python module performing space charge potential calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"space charge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fire Up the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1,  # use all available gpu\n",
    "    n_ctx=4096,  # maxing out the context window\n",
    "    temperature=0.05,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"[INST]You are helpful and respectful assistant tasked to answer user question based on a given context. \\\n",
    "Using the following pieces of retrieved context, delimited by <cntx> and </cntx>, to answer the question which is delimted by <qstn> and </qstn>. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use maximum 3 sentences. \\\n",
    "Provide the answer directly without any introduction about the context.\n",
    "\n",
    "<qstn>\n",
    "Question: {question}\n",
    "</qstn>\n",
    "\n",
    "<cntx>\n",
    "Context: {context}\n",
    "</cntx>\n",
    "\n",
    "Answer:[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"[INST]You are helpful and respectful assistant tasked to answer user question based on a given context. Using the following pieces of retrieved context, delimited by <cntx> and </cntx>, to answer the question which is delimted by <qstn> and </qstn>. If you don't know the answer, just say that you don't know. Use maximum 3 sentences. Provide the answer directly without any introduction about the context.\\n\\n<qstn>\\nQuestion: {question}\\n</qstn>\\n\\n<cntx>\\nContext: {context}\\n</cntx>\\n\\nAnswer:[/INST]\"))]\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create the chain\n",
    "\n",
    "The idea is of the chain is:\n",
    "1. User question is sent to retriever.\n",
    "2. Retriever returns documents.\n",
    "3. Format the documents joining them as the context of the query.\n",
    "4. Context + question is added to the prompt template and pushed to the LLM\n",
    "5. LLM response is StrOutputParse-d.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_question = \"What is space charge?\"\n",
    "response = rag_chain.invoke(test_user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  Based on the provided context, the answer to the question \"What is space charge?\" is:\n",
       "\n",
       "\"The space charge potential can be expressed in action angle variables with an extra dependence on the orbital angle. In this form, noted as ¯Vsc, it can be included in the Hamiltonian of Eq. (4) to obtain the perturbed Hamiltonian as: ¯H=QxJx+QyJy+¯Vsc (8) Since the space charge potential is a summation of infinite terms, the term corresponding to the resonance under study can be considered individually.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the output generation:\n",
    "def invoke_chain(user_q):\n",
    "    return rag_chain.invoke(user_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interface\n",
    "demo = gr.Interface(\n",
    "    fn=invoke_chain,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"textbox\",\n",
    "    title=\"AskMyPDF\",\n",
    "    theme=\"soft\",\n",
    "    allow_flagging=\"never\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
